Terminadas:

    -Tomar consulta
    -Limpiar consulta LLM
    -Extraer sintomas y enfemedades LLM
    -Obtener embedding ME

Falta probar:

    -Buscar documentos relacionados

Falta por documentar:

    -Obtener embedding ME
    -Buscar documentos relacionados

Falta implementar:

    -Crawler:
        -scrapper
        -buscar documentos desactualizados
        -buscar documentos que no esten aun en la base de datos
        -procesador de documentos
        -dividr en chunks

    -orquestador:
        -generador de preguntas LLM
        -decidir con metaheuristicas a que especialistas preguntarle.
        -pedir diagnosticos a los especialistas:
            -un diagnostico con los los sintomas y enfermedades de la consulta en lo que espera respuestas del usuario
            -otro diagnostico que incluya los nuevos sintomas y enfermedades.
        -generar respuestas LLM

    -especialistas
        -crear base de conocimientos
        -crear metodo de inferencia de conocimiento
        -metodo para agregar conocimiento a la base de conocimientos


========================================================================================================================

crawler_agent/
â”‚
â”œâ”€â”€ __init__.py
â”œâ”€â”€ agent.py                 # Controlador principal del agente (loop, tareas programadas)
â”‚
â”œâ”€â”€ discovery/
â”‚   â””â”€â”€ url_collector.py     # Extrae todas las URLs relevantes desde el Ã­ndice principal
â”‚
â”œâ”€â”€ scraper/
â”‚   â””â”€â”€ page_scraper.py      # Descarga y limpia el contenido HTML
â”‚
â”œâ”€â”€ processor/
â”‚   â”œâ”€â”€ extractor.py         # Extrae solo las secciones Ãºtiles del contenido
â”‚   â””â”€â”€ chunker.py           # Divide el contenido en chunks listos para embeddings
â”‚
â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ doc_store.py         # Guarda y consulta documentos con metadatos (SQLite o similar)
â”‚   â””â”€â”€ url_registry.py      # Registro de URLs visitadas con fecha
â”‚
â”œâ”€â”€ vectorstore/
â”‚   â””â”€â”€ vector_uploader.py   # Inserta/updatea chunks embebidos en la base vectorial
â”‚
â””â”€â”€ utils/
    â””â”€â”€ logger.py            # Logging estructurado y trazabilidad


[discovery] collect_urls() -> lista URLs
    â†“
[database] url_registry.url_exists() para filtrar
    â†“
[scraper] scrape_page(url) -> {url, html, raw_text, scrape_date}
    â†“
[processor] extract_relevant_sections(raw_text) -> texto limpio
    â†“
[processor] chunk_text(texto limpio) -> lista chunks
    â†“
[database] doc_store.save_document(url, titulo, fecha, texto limpio) -> doc_id
    â†“
[embedding] embed_texts(chunks) -> lista embeddings
    â†“
[vectorstore] upload_chunks(doc_id, chunks, embeddings)
    â†“
[database] url_registry.register_url(url, fecha)


 
ğŸ—ƒï¸ Base de Datos (Documental)

Podemos usar SQLite para prototipo o PostgreSQL si lo despliegas. Esquema mÃ­nimo:

Tabla: documents
id | url | tittle |	date | content

Tabla: urls_processed
url | date |
