Terminadas:

    -Tomar consulta
    -Limpiar consulta LLM
    -Extraer sintomas y enfemedades LLM
    -Obtener embedding ME

Falta probar:

    -Buscar documentos relacionados

Falta por documentar:

    -Obtener embedding ME
    -Buscar documentos relacionados

Falta implementar:

    -Crawler:
        -scrapper
        -buscar documentos desactualizados
        -buscar documentos que no esten aun en la base de datos
        -procesador de documentos
        -dividr en chunks

    -orquestador:
        -generador de preguntas LLM
        -decidir con metaheuristicas a que especialistas preguntarle.
        -pedir diagnosticos a los especialistas:
            -un diagnostico con los los sintomas y enfermedades de la consulta en lo que espera respuestas del usuario
            -otro diagnostico que incluya los nuevos sintomas y enfermedades.
        -generar respuestas LLM

    -especialistas
        -crear base de conocimientos
        -crear metodo de inferencia de conocimiento
        -metodo para agregar conocimiento a la base de conocimientos


========================================================================================================================

crawler_agent/
│
├── __init__.py
├── agent.py                 # Controlador principal del agente (loop, tareas programadas)
│
├── discovery/
│   └── url_collector.py     # Extrae todas las URLs relevantes desde el índice principal
│
├── scraper/
│   └── page_scraper.py      # Descarga y limpia el contenido HTML
│
├── processor/
│   ├── extractor.py         # Extrae solo las secciones útiles del contenido
│   └── chunker.py           # Divide el contenido en chunks listos para embeddings
│
├── database/
│   ├── doc_store.py         # Guarda y consulta documentos con metadatos (SQLite o similar)
│   └── url_registry.py      # Registro de URLs visitadas con fecha
│
├── vectorstore/
│   └── vector_uploader.py   # Inserta/updatea chunks embebidos en la base vectorial
│
└── utils/
    └── logger.py            # Logging estructurado y trazabilidad


[discovery] collect_urls() -> lista URLs
    ↓
[database] url_registry.url_exists() para filtrar
    ↓
[scraper] scrape_page(url) -> {url, html, raw_text, scrape_date}
    ↓
[processor] extract_relevant_sections(raw_text) -> texto limpio
    ↓
[processor] chunk_text(texto limpio) -> lista chunks
    ↓
[database] doc_store.save_document(url, titulo, fecha, texto limpio) -> doc_id
    ↓
[embedding] embed_texts(chunks) -> lista embeddings
    ↓
[vectorstore] upload_chunks(doc_id, chunks, embeddings)
    ↓
[database] url_registry.register_url(url, fecha)


 
🗃️ Base de Datos (Documental)

Podemos usar SQLite para prototipo o PostgreSQL si lo despliegas. Esquema mínimo:

Tabla: documents
id | url | tittle |	date | content

Tabla: urls_processed
url | date |
